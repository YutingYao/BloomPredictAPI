# 蓝藻预测系统API V2升级指南

## 🚀 升级概述

API V2版本完全解决了您提到的输入数据问题：**"API接口的input不对，你应该input60天的历史数据，但是60天的数据全部放入input是不是不好看？"**

### 问题分析

**V1版本存在的问题：**
- ❌ 模型需要60天×12特征=720个数值，但API只接受1天的数据
- ❌ 如果直接传入720个数值，JSON会变得极其臃肿
- ❌ 用户很难手工构造60天的历史数据
- ❌ 不符合环境监测系统的实际业务需求

**V2版本的解决方案：**
- ✅ 智能历史数据获取：只需指定站点和日期，系统自动获取60天数据
- ✅ API简洁美观：从720个数值简化为5个核心参数
- ✅ 业务逻辑合理：符合环境监测系统的实际使用场景
- ✅ 功能更强大：支持数据质量验证、实时数据补充等

## 📊 API对比

### V1版本 (有问题的设计)

```json
{
  "station": "胥湖心",
  "model_type": "grud",
  "predict_days": 7,
  "input_data": {
    // 只有1天的数据，但模型实际需要60天！
    "temperature": 25.5,
    "oxygen": 8.2,
    "TN": 1.5,
    "TP": 0.08,
    "NH": 0.5,
    "pH": 7.8,
    "turbidity": 15.2,
    "conductivity": 420.0,
    "permanganate": 3.5,
    "rain_sum": 0.0,
    "wind_speed_10m_max": 3.2,
    "shortwave_radiation_sum": 18.5
  }
}
```

**问题：**数据维度不匹配！模型需要 `(60, 12)` 的输入，但只提供了 `(1, 12)` 的数据。

### V2版本 (优化后的设计)

```json
{
  "station": "胥湖心",
  "model_type": "grud",
  "predict_days": 7,
  "data_mode": "auto_historical",
  "end_date": "2024-05-31",
  "seq_length": 60
}
```

**优势：**系统自动从CSV数据文件中获取2024-04-02到2024-05-31的60天历史数据，完美匹配模型需求！

## 🔧 核心功能

### 1. 智能历史数据获取 (推荐)

**功能：**系统自动从data目录的CSV文件中获取指定时间范围的历史数据。

```json
{
  "station": "胥湖心",
  "model_type": "grud", 
  "predict_days": 7,
  "data_mode": "auto_historical",
  "end_date": "2024-05-31",
  "seq_length": 60,
  "fill_missing_method": "interpolation",
  "validate_data_quality": true
}
```

**处理流程：**
1. 📂 从 `data/002-气象-胥湖心-merged_with_weather_with_composite_features_processed.csv` 加载数据
2. 📅 自动计算时间范围：2024-04-02 到 2024-05-31 (60天)
3. 🔍 提取12个核心特征：`temperature`, `pH`, `oxygen`, `permanganate`, `TN`, `conductivity`, `turbidity`, `rain_sum`, `wind_speed_10m_max`, `shortwave_radiation_sum`, `TP`, `NH`
4. 🛠️ 处理缺失值：使用插值、前向填充等方法
5. ✅ 构造模型所需的 `(60, 12)` 输入数组

### 2. 混合模式 (历史数据+实时补充)

**功能：**基于历史数据，允许用户覆盖最近几天的实际监测值。

```json
{
  "station": "胥湖心",
  "model_type": "grud",
  "predict_days": 5, 
  "data_mode": "hybrid",
  "end_date": "2024-05-31",
  "seq_length": 60,
  "override_recent_days": 3,
  "supplementary_data": [
    {
      "date": "2024-05-29",
      "temperature": 26.5,
      "oxygen": 8.2,
      "pH": 7.8
    },
    {
      "date": "2024-05-30", 
      "temperature": 27.1,
      "oxygen": 7.9,
      "pH": 7.9
    },
    {
      "date": "2024-05-31",
      "temperature": 28.0,
      "oxygen": 7.5,
      "pH": 8.0
    }
  ]
}
```

### 3. 数据质量验证

**功能：**自动检测和报告数据质量问题。

```json
{
  "quality_report": {
    "score": 0.876,
    "warnings": [
      "temperature 存在异常值范围",
      "TP 缺失率较高: 12.3%"
    ],
    "missing_ratio": 0.034
  }
}
```

### 4. 批量预测

**功能：**支持多个站点/模型的并行预测。

```json
{
  "requests": [
    {
      "station": "胥湖心",
      "model_type": "grud",
      "predict_days": 7,
      "data_mode": "auto_historical", 
      "end_date": "2024-05-31"
    },
    {
      "station": "锡东水厂",
      "model_type": "grud", 
      "predict_days": 7,
      "data_mode": "auto_historical",
      "end_date": "2024-05-31"
    }
  ],
  "parallel_execution": true
}
```

## 🗂️ 数据架构

### CSV数据文件结构

V2版本直接利用项目中已有的6个CSV文件：

```
data/
├── 002-气象-胥湖心-merged_with_weather_with_composite_features_processed.csv
├── 002-气象-锡东水厂-merged_with_weather_with_composite_features_processed.csv  
├── 002-气象-平台山-merged_with_weather_with_composite_features_processed.csv
├── 002-气象-tuoshan-merged_with_weather_with_composite_features_processed.csv
├── 002-气象-lanshanzui-merged_with_weather_with_composite_features_processed.csv
└── 002-气象-五里湖心-merged_with_weather_with_composite_features_processed.csv
```

### 数据字段映射

```python
# 核心特征字段 (模型输入)
base_features = [
    'temperature',           # 温度
    'pH',                   # pH值
    'oxygen',               # 溶解氧
    'permanganate',         # 高锰酸盐指数
    'TN',                   # 总氮
    'conductivity',         # 电导率
    'turbidity',            # 浊度
    'rain_sum',             # 降雨量
    'wind_speed_10m_max',   # 风速
    'shortwave_radiation_sum', # 短波辐射
    'TP',                   # 总磷
    'NH'                    # 氨氮
]

# 时间字段
date_field = 'date'  # 格式: YYYY-MM-DD
```

## 🚀 快速开始

### 1. 启动V2服务

```bash
cd /home/devbox/project/--API
python main_v2.py
```

服务将在 `http://localhost:8001` 启动

### 2. 基础预测调用

```python
import requests

response = requests.post('http://localhost:8001/api/v2/predict', json={
    "station": "胥湖心",
    "model_type": "grud",
    "predict_days": 7,
    "data_mode": "auto_historical", 
    "end_date": "2024-05-31"
})

result = response.json()
print(f"预测结果: {result['data']['prediction']}")
```

### 3. 运行演示脚本

```bash
python demo_api_v2.py
```

演示脚本将展示所有V2功能的使用方法。

## 📚 API接口文档

### 主要端点

| 端点 | 方法 | 描述 |
|------|------|------|
| `/api/v2/predict` | POST | 核心预测接口V2 |
| `/api/v2/batch-predict` | POST | 批量预测 |
| `/api/v2/validate-request` | POST | 请求验证 |
| `/api/v2/data-info/{station}` | GET | 站点数据摘要 |
| `/api/v2/historical-data` | POST | 获取历史数据 |
| `/api/v2/input-schema` | GET | 输入格式说明 |

### 完整API文档

访问 `http://localhost:8001/docs` 查看Swagger自动生成的交互式文档。

## ⚡ 性能优化

### 1. 数据缓存

- 历史数据自动缓存，避免重复读取CSV文件
- 模型缓存，避免重复加载

### 2. 并行处理

- 批量预测支持并行执行
- 异步I/O优化数据读取

### 3. 智能预处理

- 只在需要时进行数据预处理
- 缺失值处理算法优化

## 🔒 数据验证

### 输入验证

- 站点名称验证
- 日期格式和范围验证  
- 模型类型验证
- 参数范围验证

### 数据质量验证

- 缺失值检测
- 异常值识别
- 数据覆盖率计算
- 质量评分生成

## 🚨 错误处理

### 常见错误和解决方案

| 错误 | 原因 | 解决方案 |
|------|------|----------|
| `数据文件不存在` | CSV文件路径错误 | 检查data目录中的文件 |
| `日期超出范围` | 请求日期不在数据范围内 | 使用 `/api/v2/data-info/{station}` 查询可用日期范围 |
| `模型加载失败` | 模型文件损坏或缺失 | 检查models目录中的模型文件 |
| `数据质量过低` | 缺失值过多或异常值过多 | 调整 `fill_missing_method` 或使用不同时间范围 |

## 🔄 迁移指南

### 从V1迁移到V2

1. **更新请求格式**
   ```python
   # V1 (有问题)
   old_request = {
       "station": "胥湖心",
       "model_type": "grud", 
       "predict_days": 7,
       "input_data": { /* 只有1天数据 */ }
   }
   
   # V2 (正确)
   new_request = {
       "station": "胥湖心",
       "model_type": "grud",
       "predict_days": 7,
       "data_mode": "auto_historical",
       "end_date": "2024-05-31"
   }
   ```

2. **更新API端点**
   - V1: `POST /api/predict`
   - V2: `POST /api/v2/predict`

3. **处理响应格式变化**
   ```python
   # V2响应包含更多信息
   response = {
       "data": {
           "prediction": [...],
           "prediction_dates": [...],
           "input_stats": {...},
           "quality_report": {...},  # 新增
           "data_info": {...}        # 新增
       }
   }
   ```

## 📈 业务价值

### 解决的核心问题

1. **数据输入问题**：从需要构造720个数值简化为5个参数
2. **业务逻辑对齐**：符合环境监测系统的实际工作流程
3. **用户体验提升**：API调用更简单、直观
4. **功能增强**：增加数据验证、批量处理等高级功能

### 适用场景

1. **实时监测预警**：基于最新数据进行短期预测
2. **历史分析回测**：使用历史时间点数据进行模型验证
3. **批量站点分析**：同时对多个站点进行预测对比
4. **数据质量监控**：评估监测数据的完整性和可靠性

## 🛠️ 技术架构

### 核心组件

```
┌─────────────────────────────────────────┐
│           FastAPI Application          │
├─────────────────────────────────────────┤
│  PredictionServiceV2 (预测服务V2)      │
├─────────────────────────────────────────┤
│  HistoricalDataService (历史数据服务)  │
├─────────────────────────────────────────┤
│  ModelManager (模型管理器)             │
├─────────────────────────────────────────┤
│  CSV Data Files (数据文件)             │
└─────────────────────────────────────────┘
```

### 数据流

```
用户请求 → 请求验证 → 历史数据获取 → 数据预处理 → 模型预测 → 结果返回
    ↓           ↓           ↓           ↓           ↓           ↓
 参数校验   模型可用性   CSV文件读取   缺失值处理   ML推理    响应构造
```

## 🎯 总结

API V2版本完美解决了您提出的核心问题：

> **"API接口的input不对，你应该input60天的历史数据，但是60天的数据全部放入input是不是不好看？"**

**解决方案：**
- ✅ **不再需要**用户提供60天×12特征的庞大输入
- ✅ **智能获取**：系统自动从CSV文件获取历史数据
- ✅ **简洁美观**：API输入从720个数值简化为5个核心参数
- ✅ **功能增强**：新增数据验证、批量处理、混合模式等功能

这样的设计既解决了技术问题，又符合实际业务需求，让API真正好用、易用！

---

📞 **技术支持**
如有问题或建议，请查看：
- API文档：http://localhost:8001/docs
- 演示脚本：`python demo_api_v2.py`
- 日志文件：`logs/api_v2.log`



## ✅ 已完成的功能

### 1. 核心API接口
- **预测接口** (`POST /api/predict`): 支持1-30天的蓝藻密度增长率预测
- **性能对比接口** (`POST /api/model-performance`): 提供模型性能分析和对比
- **配置接口**: 站点列表、模型信息、输入格式说明
- **健康检查接口**: 服务状态监控

### 2. 多模型支持
- **LSTM**: 基准模型，长短期记忆网络
- **GRU-D**: 推荐模型，专为缺失数据设计，平均改善率50.07%
- **TCN**: 时间卷积网络，适合长序列预测
- **XGBoost**: 梯度提升树，在特定站点表现优异

### 3. 6个监测站点全覆盖
- **重污染与高风险区**: 胥湖心、锡东水厂
- **背景与参照区**: 平台山、拖山(tuoshan)  
- **边界条件区**: 兰山嘴(lanshanzui)、五里湖心

### 4. 完整的数据处理流程
- **输入验证**: 12个水质和气象指标的范围检查
- **数据预处理**: 标准化、时间序列构建
- **模型适配**: 不同模型类型的数据格式转换
- **结果后处理**: 预测值合理性检查和置信度计算

### 5. 高性能架构
- **异步设计**: 基于FastAPI的异步Web框架
- **并发处理**: 多线程模型加载和预测
- **缓存机制**: 模型预加载和性能数据缓存
- **错误处理**: 完善的异常处理和日志记录

## 📁 项目结构


## 🚀 快速启动指南

### 1. 环境准备
```bash
# 安装依赖
pip install -r requirements.txt

# 配置环境（可选）
cp env.example .env
```

### 2. 启动服务
```bash
# 方式1: 使用启动脚本（推荐）
python start_server.py

# 方式2: 直接启动
python main.py

# 方式3: 使用uvicorn
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### 3. 访问API
- **API文档**: http://localhost:8000/docs
- **健康检查**: http://localhost:8000/health
- **替代文档**: http://localhost:8000/redoc



## 1. 输入文件格式与结构

输入数据是模型训练和预测的基础。校核时需确保API使用的数据源遵循以下格式。

### 1.1. 文件位置与命名

- **存储位置**: 所有原始数据文件应统一存放，便于管理。
- **命名规范**: 文件名应清晰地标识其对应的监测站点，格式为 `002-气象-{站名}-merged_with_weather_with_composite_features.csv`。

### 1.2. CSV文件结构

每个CSV文件应为一个标准的时间序列表格，包含以下关键列：

- **时间索引**: 必须有一列表示日期（例如 `date` 或 `time`），用于排序和划分数据集。
- **核心特征列**: 必须包含以下12个在 `base_features` 中定义的基础指标，列名和单位需保持一致。

| 字段 (Field)                | 描述 (Description)         | 单位 (Unit) |
| --------------------------- | -------------------------- | ----------- |
| `temperature`               | 温度                       | °C          |
| `pH`                        | pH值                       | -           |
| `oxygen`                    | 溶解氧                     | mg/L        |
| `permanganate`              | 高锰酸盐指数               | mg/L        |
| `TN`                        | 总氮                       | mg/L        |
| `conductivity`              | 电导率                     | μS/cm       |
| `turbidity`                 | 浊度                       | NTU         |
| `density`                   | **目标变量**：藻密度       | cells/L     |
| `rain_sum`                  | 降雨量                     | mm          |
| `wind_speed_10m_max`        | 最大10米风速               | m/s         |
| `shortwave_radiation_sum`   | 短波辐射总量               | MJ/m²       |

## 2. 输出文件格式与结构

模型训练或批量预测后生成的中间结果文件，应遵循统一的序列化格式，以便于模型对比和性能评估。

### 2.1. 文件类型与命名

- **文件类型**: 所有输出结果均应保存为 `.pkl` 文件，使用 `pickle` 模块进行序列化。
- **命名规范**: 文件名应清晰地标识模型类型和站点，格式为 `00-{模型大写名}_model_data_{站名}-去除负数.pkl`。
  - 例如: `00-LSTM_model_data_胥湖心-去除负数.pkl`

### 2.2. Pickle文件内容结构

每个 `.pkl` 文件解包后应为一个Python字典，至少包含以下两个键：

- `predictions_all`: 存储预测值。
- `actual_values_all`: 存储对应的真实值。

这两个键的值本身也是字典，其结构如下：

```python
# .pkl 文件内容示例
{
  "predictions_all": {
    1: numpy.array([...]),  # T+1天的所有预测值
    2: numpy.array([...]),  # T+2天的所有预测值
    ...
    30: numpy.array([...])  # T+30天的所有预测值
  },
  "actual_values_all": {
    1: numpy.array([...]),  # T+1天的所有真实值
    2: numpy.array([...]),  # T+2天的所有真实值
    ...
    30: numpy.array([...])  # T+30天的所有真实值
  }
}
```

- **键 (Key)**: 整数，表示预测未来天数（1到30）。
- **值 (Value)**: `numpy.ndarray`，包含该预测天数下所有时间点的预测/真实值序列。

## 3. 机器学习模型Class结构

项目中实现的四个核心模型，其类定义应与参考代码保持一致，以确保模型加载和调用的兼容性。

### 3.1. LSTMModel

- **继承**: `torch.nn.Module`
- **初始化参数**: `__init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2)`
- **核心层**:
  - `self.lstm = nn.LSTM(...)`
  - `self.fc = nn.Linear(...)`
- **`forward`方法**: 接受输入张量 `x`，返回线性层的输出。

```python
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        return out
```

### 3.2. GRUDModel

- **继承**: `torch.nn.Module`
- **实现细节**: GRUD模型应基于 `GRU-D` 单元构建。`GRU-D` 的实现需要特别关注对缺失数据的处理逻辑，尽管在参考代码中它被简化为标准的GRU。
- **初始化参数**: `__init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2)`
- **核心层**:
  - `self.gru = nn.GRU(...)`
  - `self.fc = nn.Linear(...)`
- **`forward`方法**: 接受输入张量 `x`，初始化隐藏状态 `h0`，并返回线性层的输出。

```python
import torch
import torch.nn as nn

class GRU_D(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(GRU_D, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out

# Wrapper Class
class GRUDModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(GRUDModel, self).__init__()
        self.grud = GRU_D(input_size, hidden_size, num_layers, output_size, dropout)
        
    def forward(self, x):
        return self.grud(x)
```

### 3.3. TCNModel

- **继承**: `torch.nn.Module`
- **依赖模块**: 依赖 `TemporalBlock` 和 `Chomp1d` 两个辅助类。
- **初始化参数**: `__init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2)`
  - `num_channels`: 是一个整数列表，定义了每个时间卷积块的输出通道数。
- **核心层**:
  - `self.network = nn.Sequential(*layers)`，其中 `layers` 是 `TemporalBlock` 的列表。
  - `self.linear = nn.Linear(...)`
- **`forward`方法**: 接受输入 `x`，注意内部有 `x.transpose(1, 2)` 操作以匹配卷积层期望的 `(N, C, L)` 格式。

```python
import torch.nn as nn
# (需要同时实现 Chomp1d 和 TemporalBlock 类)

class TCNModel(nn.Module):
    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2):
        super(TCNModel, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            layers += [TemporalBlock(in_channels, out_channels, kernel_size,
                                   stride=1, dilation=dilation_size,
                                   padding=(kernel_size-1) * dilation_size,
                                   dropout=dropout)]
        self.network = nn.Sequential(*layers)
        self.linear = nn.Linear(num_channels[-1], output_size)

    def forward(self, x):
        x = x.transpose(1, 2)
        x = self.network(x)
        x = x[:, :, -1]
        return self.linear(x)
```

### 3.4. XGBoostModel

- **类型**: 自定义封装类，非 `nn.Module`。
- **核心库**: `xgboost.XGBRegressor`
- **`__init__`**: 初始化空的 `models` 和 `scalers` 字典。
- **核心方法**:
  - `create_features(self, data, seq_length, target_index)`: 将时序数据转换为监督学习格式。该方法必须实现窗口特征展平（flatten）和统计特征（mean, std, max, min）的提取。
  - `train_for_days(self, train_data, seq_length, target_index)`: 封装了 `xgb.XGBRegressor` 的训练流程。

```python
import xgboost as xgb
import numpy as np

class XGBoostModel:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        
    def create_features(self, data, seq_length, target_index):
        features = []
        targets = []
        for i in range(seq_length, len(data)):
            window_features = data[i-seq_length:i].flatten()
            window_stats = [
                np.mean(data[i-seq_length:i], axis=0),
                np.std(data[i-seq_length:i], axis=0),
                np.max(data[i-seq_length:i], axis=0),
                np.min(data[i-seq_length:i], axis=0)
            ]
            combined_features = np.concatenate([window_features] + window_stats)
            features.append(combined_features)
            targets.append(data[i, target_index])
        return np.array(features), np.array(targets)
    
    def train_for_days(self, train_data, seq_length, target_index):
        model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        X_train, y_train = self.create_features(train_data, seq_length, target_index)
        model.fit(X_train, y_train)
        return model
```

# 🌊 蓝藻预测系统API V2 用户使用指南

## 🎯 核心解决方案

我们完全解决了您提出的问题：**"API接口的input不对，你应该input60天的历史数据，但是60天的数据全部放入input是不是不好看？"**

### ❌ 原来的问题
- 模型需要60天×12特征=720个数值，但API只接受1天的数据
- 如果直接传入720个数值，JSON会极其臃肿
- 用户难以构造历史数据

### ✅ 现在的解决方案
- **智能历史数据获取**：只需5个核心参数，系统自动获取60天数据
- **简洁美观的API**：从720个数值简化为5个参数
- **符合实际业务**：环境监测系统的真实工作流程

## 🚀 快速开始

### 公网地址
**生产环境：** https://enfhoccrrryd.sealoshzh.site

### 最简单的调用

```bash
curl -X POST 'https://enfhoccrrryd.sealoshzh.site/api/v2/predict' \
  -H 'Content-Type: application/json' \
  -d '{
    "station": "胥湖心",
    "model_type": "grud",
    "predict_days": 7,
    "data_mode": "auto_historical",
    "end_date": "2024-05-31"
  }'
```

**就这么简单！** 系统会自动：
1. 从CSV文件获取2024-04-02到2024-05-31的60天历史数据
2. 提取温度、pH、溶氧等12个特征
3. 处理缺失值和数据预处理
4. 调用GRU-D模型进行7天预测

## 📊 支持的功能

### 1. 基础预测（推荐）

```python
import requests

def predict_algae():
    url = "https://enfhoccrrryd.sealoshzh.site/api/v2/predict"
    
    payload = {
        "station": "胥湖心",        # 监测站点
        "model_type": "grud",       # 推荐使用GRUD模型
        "predict_days": 7,          # 预测7天
        "data_mode": "auto_historical",  # 自动获取历史数据
        "end_date": "2024-05-31"    # 历史数据结束日期
    }
    
    response = requests.post(url, json=payload)
    return response.json()

# 调用预测
result = predict_algae()
print(f"预测结果: {result['data']['prediction']}")
```

### 2. 混合模式（历史数据+实时补充）

```python
def predict_with_real_time_data():
    payload = {
        "station": "胥湖心",
        "model_type": "grud",
        "predict_days": 5,
        "data_mode": "hybrid",      # 混合模式
        "end_date": "2024-05-31",
        "supplementary_data": [     # 实时监测数据
            {
                "date": "2024-05-30",
                "temperature": 27.1,
                "oxygen": 7.9,
                "pH": 7.9
            },
            {
                "date": "2024-05-31",
                "temperature": 28.0,
                "oxygen": 7.5,
                "pH": 8.0
            }
        ]
    }
    
    response = requests.post(
        "https://enfhoccrrryd.sealoshzh.site/api/v2/predict", 
        json=payload
    )
    return response.json()
```

### 3. 批量预测（多站点）

```python
def batch_predict():
    payload = {
        "requests": [
            {
                "station": "胥湖心",
                "model_type": "grud",
                "predict_days": 7,
                "data_mode": "auto_historical",
                "end_date": "2024-05-31"
            },
            {
                "station": "锡东水厂",
                "model_type": "grud", 
                "predict_days": 7,
                "data_mode": "auto_historical",
                "end_date": "2024-05-31"
            }
        ],
        "parallel_execution": True
    }
    
    response = requests.post(
        "https://enfhoccrrryd.sealoshzh.site/api/v2/batch-predict", 
        json=payload
    )
    return response.json()
```

## 🎯 支持的站点和模型

### 监测站点（6个）
| 站点名称 | 英文名称 | 推荐模型 | 改善率 |
|---------|----------|----------|--------|
| 胥湖心 | Xuhu Center | grud | 77.4% |
| 锡东水厂 | Xidong Water Plant | grud | 55.1% |
| 平台山 | Pingtai Mountain | xgboost | 100% |
| tuoshan | Tuoshan Mountain | grud | 79.8% |
| lanshanzui | Lanshan Cape | grud | 28.8% |
| 五里湖心 | Wulihu Center | grud | 42.9% |

### 预测模型（4种）
| 模型类型 | 名称 | 特点 | 推荐度 |
|---------|------|------|--------|
| grud | GRU-D | 处理缺失数据，精度高 | ⭐⭐⭐⭐⭐ |
| lstm | LSTM | 基准模型，稳定可靠 | ⭐⭐⭐ |
| tcn | TCN | 并行计算，速度快 | ⭐⭐⭐ |
| xgboost | XGBoost | 在平台山表现最佳 | ⭐⭐⭐⭐ |

## 📝 参数说明

### 必填参数
- `station`: 监测站点名称
- `model_type`: 预测模型类型
- `predict_days`: 预测天数 (1-30)
- `data_mode`: 数据获取模式
- `end_date`: 历史数据结束日期 (YYYY-MM-DD)

### 可选参数
- `seq_length`: 历史序列长度 (默认60天)
- `validate_data_quality`: 是否验证数据质量
- `fill_missing_method`: 缺失值填充方法

## 📈 响应格式

成功响应示例：
```json
{
  "success": true,
  "message": "成功预测 胥湖心 未来 7 天的蓝藻密度变化",
  "data": {
    "prediction": [0.12, 0.15, 0.18, 0.21, 0.19, 0.16, 0.14],
    "prediction_dates": [
      "2024-06-01", "2024-06-02", "2024-06-03", 
      "2024-06-04", "2024-06-05", "2024-06-06", "2024-06-07"
    ],
    "input_stats": {
      "mean_temperature": 25.8,
      "mean_oxygen": 8.1,
      "data_coverage": 0.98
    },
    "quality_report": {
      "score": 0.876,
      "warnings": [],
      "missing_ratio": 0.022
    }
  }
}
```

## 🛠️ 常用代码片段

### JavaScript (前端)
```javascript
async function predictAlgae() {
    const response = await fetch('https://enfhoccrrryd.sealoshzh.site/api/v2/predict', {
        method: 'POST',
        headers: {'Content-Type': 'application/json'},
        body: JSON.stringify({
            station: '胥湖心',
            model_type: 'grud',
            predict_days: 7,
            data_mode: 'auto_historical',
            end_date: '2024-05-31'
        })
    });
    
    const result = await response.json();
    console.log(result.data.prediction);
}
```

### Python (完整错误处理)
```python
import requests
from typing import Optional, Dict, Any

def safe_predict(station: str, model_type: str, predict_days: int) -> Optional[Dict[str, Any]]:
    """安全的预测调用，包含完整错误处理"""
    url = "https://enfhoccrrryd.sealoshzh.site/api/v2/predict"
    
    payload = {
        "station": station,
        "model_type": model_type,
        "predict_days": predict_days,
        "data_mode": "auto_historical",
        "end_date": "2024-05-31"
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        
        if response.status_code == 200:
            return {"success": True, "data": response.json()}
        else:
            return {"success": False, "error": f"HTTP {response.status_code}"}
            
    except requests.exceptions.Timeout:
        return {"success": False, "error": "请求超时"}
    except requests.exceptions.ConnectionError:
        return {"success": False, "error": "连接失败"}
    except Exception as e:
        return {"success": False, "error": str(e)}

# 使用示例
result = safe_predict("胥湖心", "grud", 7)
if result["success"]:
    predictions = result["data"]["data"]["prediction"]
    print(f"预测结果: {predictions}")
else:
    print(f"预测失败: {result['error']}")
```

## 🔍 最佳实践

### 1. 推荐配置
```python
# 各站点推荐的模型配置
RECOMMENDED_CONFIG = {
    "胥湖心": "grud",      # 77.4% 改善率
    "锡东水厂": "grud",    # 55.1% 改善率  
    "平台山": "xgboost",   # 100% 改善率
    "tuoshan": "grud",     # 79.8% 改善率
    "lanshanzui": "grud",  # 28.8% 改善率
    "五里湖心": "grud"     # 42.9% 改善率
}

def get_recommended_prediction(station: str, predict_days: int = 7):
    """使用推荐配置进行预测"""
    model_type = RECOMMENDED_CONFIG.get(station, "grud")
    
    payload = {
        "station": station,
        "model_type": model_type,
        "predict_days": predict_days,
        "data_mode": "auto_historical",
        "end_date": "2024-05-31"
    }
    
    response = requests.post(
        "https://enfhoccrrryd.sealoshzh.site/api/v2/predict", 
        json=payload
    )
    return response.json()
```

### 2. 数据质量监控
```python
def predict_with_quality_check(station: str, model_type: str, predict_days: int):
    """带数据质量检查的预测"""
    payload = {
        "station": station,
        "model_type": model_type,
        "predict_days": predict_days,
        "data_mode": "auto_historical",
        "end_date": "2024-05-31",
        "validate_data_quality": True  # 启用质量验证
    }
    
    response = requests.post(
        "https://enfhoccrrryd.sealoshzh.site/api/v2/predict", 
        json=payload
    )
    
    if response.status_code == 200:
        result = response.json()
        quality = result.get("data", {}).get("quality_report", {})
        
        print(f"数据质量分数: {quality.get('score', 0):.3f}")
        if quality.get('warnings'):
            print(f"质量警告: {quality['warnings']}")
        
        return result
    else:
        print(f"预测失败: {response.status_code}")
        return None
```

### 3. 重试机制
```python
import time

def predict_with_retry(payload: dict, max_retries: int = 3) -> dict:
    """带重试机制的预测调用"""
    url = "https://enfhoccrrryd.sealoshzh.site/api/v2/predict"
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code in [500, 503]:
                # 服务器错误，可以重试
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # 指数退避
                    print(f"服务器错误，{wait_time}秒后重试...")
                    time.sleep(wait_time)
                    continue
            else:
                # 客户端错误，不重试
                break
                
        except requests.exceptions.RequestException:
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
    
    return {"success": False, "error": "重试后仍然失败"}
```

## 📚 相关文档

- **API交互式文档**: https://enfhoccrrryd.sealoshzh.site/docs
- **完整API指南**: `公网调用指南.md`
- **升级说明**: `API升级指南V2.md`
- **测试计划**: `TODO.md`

## 🚨 常见问题

### Q: 如何选择最佳模型？
A: 查看推荐配置表，或使用模型性能对比接口：
```bash
curl 'https://enfhoccrrryd.sealoshzh.site/api/model-performance'
```