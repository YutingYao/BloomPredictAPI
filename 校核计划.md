# 项目校核计划

本文档旨在为蓝藻预测系统的后端API项目提供一套全面的校核标准。该计划基于 `参考代码.md` 文件中的实现，确保项目在数据格式、模型结构和核心逻辑上与设计保持一致。

## 1. 输入文件格式与结构

输入数据是模型训练和预测的基础。校核时需确保API使用的数据源遵循以下格式。

### 1.1. 文件位置与命名

- **存储位置**: 所有原始数据文件应统一存放，便于管理。
- **命名规范**: 文件名应清晰地标识其对应的监测站点，格式为 `002-气象-{站名}-merged_with_weather_with_composite_features.csv`。

### 1.2. CSV文件结构

每个CSV文件应为一个标准的时间序列表格，包含以下关键列：

- **时间索引**: 必须有一列表示日期（例如 `date` 或 `time`），用于排序和划分数据集。
- **核心特征列**: 必须包含以下12个在 `base_features` 中定义的基础指标，列名和单位需保持一致。

| 字段 (Field)                | 描述 (Description)         | 单位 (Unit) |
| --------------------------- | -------------------------- | ----------- |
| `temperature`               | 温度                       | °C          |
| `pH`                        | pH值                       | -           |
| `oxygen`                    | 溶解氧                     | mg/L        |
| `permanganate`              | 高锰酸盐指数               | mg/L        |
| `TN`                        | 总氮                       | mg/L        |
| `conductivity`              | 电导率                     | μS/cm       |
| `turbidity`                 | 浊度                       | NTU         |
| `density`                   | **目标变量**：藻密度       | cells/L     |
| `rain_sum`                  | 降雨量                     | mm          |
| `wind_speed_10m_max`        | 最大10米风速               | m/s         |
| `shortwave_radiation_sum`   | 短波辐射总量               | MJ/m²       |

## 2. 输出文件格式与结构

模型训练或批量预测后生成的中间结果文件，应遵循统一的序列化格式，以便于模型对比和性能评估。

### 2.1. 文件类型与命名

- **文件类型**: 所有输出结果均应保存为 `.pkl` 文件，使用 `pickle` 模块进行序列化。
- **命名规范**: 文件名应清晰地标识模型类型和站点，格式为 `00-{模型大写名}_model_data_{站名}-去除负数.pkl`。
  - 例如: `00-LSTM_model_data_胥湖心-去除负数.pkl`

### 2.2. Pickle文件内容结构

每个 `.pkl` 文件解包后应为一个Python字典，至少包含以下两个键：

- `predictions_all`: 存储预测值。
- `actual_values_all`: 存储对应的真实值。

这两个键的值本身也是字典，其结构如下：

```python
# .pkl 文件内容示例
{
  "predictions_all": {
    1: numpy.array([...]),  # T+1天的所有预测值
    2: numpy.array([...]),  # T+2天的所有预测值
    ...
    30: numpy.array([...])  # T+30天的所有预测值
  },
  "actual_values_all": {
    1: numpy.array([...]),  # T+1天的所有真实值
    2: numpy.array([...]),  # T+2天的所有真实值
    ...
    30: numpy.array([...])  # T+30天的所有真实值
  }
}
```

- **键 (Key)**: 整数，表示预测未来天数（1到30）。
- **值 (Value)**: `numpy.ndarray`，包含该预测天数下所有时间点的预测/真实值序列。

## 3. 机器学习模型Class结构

项目中实现的四个核心模型，其类定义应与参考代码保持一致，以确保模型加载和调用的兼容性。

### 3.1. LSTMModel

- **继承**: `torch.nn.Module`
- **初始化参数**: `__init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2)`
- **核心层**:
  - `self.lstm = nn.LSTM(...)`
  - `self.fc = nn.Linear(...)`
- **`forward`方法**: 接受输入张量 `x`，返回线性层的输出。

```python
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, 
                           batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = self.fc(lstm_out[:, -1, :])
        return out
```

### 3.2. GRUDModel

- **继承**: `torch.nn.Module`
- **实现细节**: GRUD模型应基于 `GRU-D` 单元构建。`GRU-D` 的实现需要特别关注对缺失数据的处理逻辑，尽管在参考代码中它被简化为标准的GRU。
- **初始化参数**: `__init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2)`
- **核心层**:
  - `self.gru = nn.GRU(...)`
  - `self.fc = nn.Linear(...)`
- **`forward`方法**: 接受输入张量 `x`，初始化隐藏状态 `h0`，并返回线性层的输出。

```python
import torch
import torch.nn as nn

class GRU_D(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(GRU_D, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out

# Wrapper Class
class GRUDModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(GRUDModel, self).__init__()
        self.grud = GRU_D(input_size, hidden_size, num_layers, output_size, dropout)
        
    def forward(self, x):
        return self.grud(x)
```

### 3.3. TCNModel

- **继承**: `torch.nn.Module`
- **依赖模块**: 依赖 `TemporalBlock` 和 `Chomp1d` 两个辅助类。
- **初始化参数**: `__init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2)`
  - `num_channels`: 是一个整数列表，定义了每个时间卷积块的输出通道数。
- **核心层**:
  - `self.network = nn.Sequential(*layers)`，其中 `layers` 是 `TemporalBlock` 的列表。
  - `self.linear = nn.Linear(...)`
- **`forward`方法**: 接受输入 `x`，注意内部有 `x.transpose(1, 2)` 操作以匹配卷积层期望的 `(N, C, L)` 格式。

```python
import torch.nn as nn
# (需要同时实现 Chomp1d 和 TemporalBlock 类)

class TCNModel(nn.Module):
    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2):
        super(TCNModel, self).__init__()
        layers = []
        num_levels = len(num_channels)
        for i in range(num_levels):
            dilation_size = 2 ** i
            in_channels = input_size if i == 0 else num_channels[i-1]
            out_channels = num_channels[i]
            layers += [TemporalBlock(in_channels, out_channels, kernel_size,
                                   stride=1, dilation=dilation_size,
                                   padding=(kernel_size-1) * dilation_size,
                                   dropout=dropout)]
        self.network = nn.Sequential(*layers)
        self.linear = nn.Linear(num_channels[-1], output_size)

    def forward(self, x):
        x = x.transpose(1, 2)
        x = self.network(x)
        x = x[:, :, -1]
        return self.linear(x)
```

### 3.4. XGBoostModel

- **类型**: 自定义封装类，非 `nn.Module`。
- **核心库**: `xgboost.XGBRegressor`
- **`__init__`**: 初始化空的 `models` 和 `scalers` 字典。
- **核心方法**:
  - `create_features(self, data, seq_length, target_index)`: 将时序数据转换为监督学习格式。该方法必须实现窗口特征展平（flatten）和统计特征（mean, std, max, min）的提取。
  - `train_for_days(self, train_data, seq_length, target_index)`: 封装了 `xgb.XGBRegressor` 的训练流程。

```python
import xgboost as xgb
import numpy as np

class XGBoostModel:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        
    def create_features(self, data, seq_length, target_index):
        features = []
        targets = []
        for i in range(seq_length, len(data)):
            window_features = data[i-seq_length:i].flatten()
            window_stats = [
                np.mean(data[i-seq_length:i], axis=0),
                np.std(data[i-seq_length:i], axis=0),
                np.max(data[i-seq_length:i], axis=0),
                np.min(data[i-seq_length:i], axis=0)
            ]
            combined_features = np.concatenate([window_features] + window_stats)
            features.append(combined_features)
            targets.append(data[i, target_index])
        return np.array(features), np.array(targets)
    
    def train_for_days(self, train_data, seq_length, target_index):
        model = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.1,
            random_state=42
        )
        X_train, y_train = self.create_features(train_data, seq_length, target_index)
        model.fit(X_train, y_train)
        return model
```
```